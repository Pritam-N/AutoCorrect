{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = no.zeros((5,5), no.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    dist[i][0] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0],\n       [2, 0, 0, 0, 0],\n       [3, 0, 0, 0, 0],\n       [4, 0, 0, 0, 0]])"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 5):\n",
    "    dist[0][i] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "dist[-1][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package brown to nltk-data...\n[nltk_data]   Unzipping corpora\\brown.zip.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "nltk.download('brown', download_dir='nltk-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "INFO:absl:Using D:\\Temp\\tfhub_modules to cache modules.\nINFO:absl:Downloading TF-Hub Module 'https://tfhub.dev/google/elmo/3'.\nINFO:absl:Downloading https://tfhub.dev/google/elmo/3: 100.34MB\nINFO:absl:Downloading https://tfhub.dev/google/elmo/3: 170.34MB\nINFO:absl:Downloading https://tfhub.dev/google/elmo/3: 240.34MB\nINFO:absl:Downloading https://tfhub.dev/google/elmo/3: 320.34MB\nINFO:absl:Downloaded https://tfhub.dev/google/elmo/3, Total size: 357.40MB\nINFO:absl:Downloaded TF-Hub Module 'https://tfhub.dev/google/elmo/3'.\n"
    },
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled.",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-caced7ee1735>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0melmo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://tfhub.dev/google/elmo/3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\module.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, spec, trainable, name, tags)\u001b[0m\n\u001b[0;32m    179\u001b[0m           \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m           tags=self._tags)\n\u001b[0m\u001b[0;32m    182\u001b[0m       \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m_create_impl\u001b[1;34m(self, name, trainable, tags)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_variables_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_export\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables_saver\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, spec, meta_graph, trainable, checkpoint_path, name)\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;31m# TPU training code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mscope_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 444\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m_init_state\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_init_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m     \u001b[0mvariable_tensor_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state_map\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_state_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m     self._variable_map = recover_partitioned_variable_map(\n\u001b[0;32m    449\u001b[0m         get_node_map_from_tensor_map(variable_tensor_map))\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_hub\\native_module.py\u001b[0m in \u001b[0;36m_create_state_graph\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    502\u001b[0m         \u001b[0mmeta_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m         \u001b[0minput_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 504\u001b[1;33m         import_scope=relative_scope_name)\n\u001b[0m\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m     \u001b[1;31m# Build a list from the variable name in the module definition to the actual\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[0;32m   1460\u001b[0m   return _import_meta_graph_with_return_elements(meta_graph_or_file,\n\u001b[0;32m   1461\u001b[0m                                                  \u001b[0mclear_devices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimport_scope\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1462\u001b[1;33m                                                  **kwargs)[0]\n\u001b[0m\u001b[0;32m   1463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[1;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m   \u001b[1;34m\"\"\"Import MetaGraph, and return both a saver and returned elements.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m     raise RuntimeError(\"Exporting/importing meta graphs is not supported when \"\n\u001b[0m\u001b[0;32m   1473\u001b[0m                        \u001b[1;34m\"eager execution is enabled. No graph exists when eager \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m                        \"execution is enabled.\")\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Exporting/importing meta graphs is not supported when eager execution is enabled. No graph exists when eager execution is enabled."
     ]
    }
   ],
   "source": [
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "text": "\u001b[1;31mSignature:\u001b[0m \u001b[0mhub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mSource:\u001b[0m   \n\u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[1;34m\"\"\"Resolves a handle and loads the resulting module.\n\n  This is the preferred API to load a Hub module in low-level TensorFlow 2.\n  Users of higher-level frameworks like Keras should use the framework's\n  corresponding wrapper, like hub.KerasLayer.\n\n  This function is roughly equivalent to the TF2 function\n  `tf.saved_model.load()` on the result of `hub.resolve(handle)`. Calling this\n  function requires TF 1.14 or newer. It can be called both in eager and graph\n  mode.\n\n  Note: Using in a tf.compat.v1.Session with variables placed on parameter\n  servers requires setting `experimental.share_cluster_devices_in_session`\n  within the `tf.compat.v1.ConfigProto`. (It becomes non-experimental in TF2.2.)\n\n  This function can handle the deprecated TF1 Hub format to the extent\n  that `tf.saved_model.load()` in TF2 does. In particular, the returned object\n  has attributes\n    * `.variables`: a list of variables from the loaded object;\n    * `.signatures`: a dict of TF2 ConcreteFunctions, keyed by signature names,\n      that take tensor kwargs and return a tensor dict.\n  However, the information imported by hub.Module into the collections of a\n  tf.Graph is lost (e.g., regularization losses and update ops).\n\n  Args:\n    handle: (string) the Module handle to resolve; see hub.resolve().\n    tags: A set of strings specifying the graph variant to use, if loading from\n      a v1 module.\n    options: Optional, `tf.saved_model.LoadOptions` object that specifies\n      options for loading. This argument can only be used from TensorFlow 2.3\n      onwards.\n\n  Returns:\n    A trackable object (see tf.saved_model.load() documentation for details).\n\n  Raises:\n    NotImplementedError: If the code is running against incompatible (1.x)\n                         version of TF.\n  \"\"\"\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_v1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"load_v2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"hub.load() is not implemented for TF < 1.14.x, \"\u001b[0m\u001b[1;33m\n\u001b[0m                              \u001b[1;34m\"Current version: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Expected a string, got %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[0mmodule_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[0mis_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n\u001b[0m      \u001b[0mnative_module\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_module_proto_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0mtags\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mtags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\n\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[1;32mif\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"saved_model\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LoadOptions\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n\u001b[0m      \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"options are not supported for TF < 2.3.x,\"\u001b[0m\u001b[1;33m\n\u001b[0m                                \u001b[1;34m\" Current version: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_v1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n\u001b[0m        \u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\n\u001b[0m    \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_v1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_hub_module_v1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mis_hub_module_v1\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\n\u001b[0m  \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;31mFile:\u001b[0m      d:\\programdata\\anaconda3\\lib\\site-packages\\tensorflow_hub\\module_v2.py\n\u001b[1;31mType:\u001b[0m      function\n"
    }
   ],
   "source": [
    "hub.load??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "'wget' is not recognized as an internal or external command,\noperable program or batch file.\n"
    }
   ],
   "source": [
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "2"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "x = 'my name'\n",
    "len(x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from logging import error\n",
    "from os import replace\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "40M [01:29<00:37, 2.91MB/s]\u001b[A\nDownloading:  75%|███████▌  | 331M/440M [01:29<00:34, 3.18MB/s]\u001b[A\nDownloading:  75%|███████▌  | 331M/440M [01:29<00:33, 3.27MB/s]\u001b[A\nDownloading:  75%|███████▌  | 332M/440M [01:29<00:30, 3.58MB/s]\u001b[A\nDownloading:  75%|███████▌  | 332M/440M [01:29<00:27, 3.93MB/s]\u001b[A\nDownloading:  76%|███████▌  | 333M/440M [01:29<00:24, 4.39MB/s]\u001b[A\nDownloading:  76%|███████▌  | 333M/440M [01:29<00:24, 4.36MB/s]\u001b[A\nDownloading:  76%|███████▌  | 334M/440M [01:29<00:24, 4.42MB/s]\u001b[A\nDownloading:  76%|███████▌  | 334M/440M [01:29<00:23, 4.61MB/s]\u001b[A\nDownloading:  76%|███████▌  | 335M/440M [01:30<00:24, 4.27MB/s]\u001b[A\nDownloading:  76%|███████▌  | 335M/440M [01:30<00:25, 4.05MB/s]\u001b[A\nDownloading:  76%|███████▌  | 336M/440M [01:30<00:24, 4.26MB/s]\u001b[A\nDownloading:  76%|███████▋  | 336M/440M [01:30<00:22, 4.58MB/s]\u001b[A\nDownloading:  76%|███████▋  | 337M/440M [01:30<00:22, 4.62MB/s]\u001b[A\nDownloading:  77%|███████▋  | 337M/440M [01:30<00:24, 4.19MB/s]\u001b[A\nDownloading:  77%|███████▋  | 338M/440M [01:30<00:24, 4.22MB/s]\u001b[A\nDownloading:  77%|███████▋  | 338M/440M [01:30<00:23, 4.37MB/s]\u001b[A\nDownloading:  77%|███████▋  | 339M/440M [01:31<00:28, 3.56MB/s]\u001b[A\nDownloading:  77%|███████▋  | 340M/440M [01:31<00:23, 4.33MB/s]\u001b[A\nDownloading:  77%|███████▋  | 340M/440M [01:31<00:20, 4.78MB/s]\u001b[A\nDownloading:  77%|███████▋  | 341M/440M [01:31<00:22, 4.47MB/s]\u001b[A\nDownloading:  77%|███████▋  | 341M/440M [01:31<00:23, 4.30MB/s]\u001b[A\nDownloading:  78%|███████▊  | 342M/440M [01:31<00:21, 4.55MB/s]\u001b[A\nDownloading:  78%|███████▊  | 342M/440M [01:31<00:22, 4.36MB/s]\u001b[A\nDownloading:  78%|███████▊  | 343M/440M [01:31<00:22, 4.41MB/s]\u001b[A\nDownloading:  78%|███████▊  | 343M/440M [01:31<00:21, 4.56MB/s]\u001b[A\nDownloading:  78%|███████▊  | 344M/440M [01:32<00:29, 3.25MB/s]\u001b[A\nDownloading:  78%|███████▊  | 344M/440M [01:32<00:38, 2.51MB/s]\u001b[A\nDownloading:  78%|███████▊  | 345M/440M [01:32<00:33, 2.83MB/s]\u001b[A\nDownloading:  78%|███████▊  | 345M/440M [01:32<00:31, 3.02MB/s]\u001b[A\nDownloading:  78%|███████▊  | 346M/440M [01:32<00:28, 3.38MB/s]\u001b[A\nDownloading:  79%|███████▊  | 346M/440M [01:32<00:29, 3.18MB/s]\u001b[A\nDownloading:  79%|███████▊  | 347M/440M [01:33<00:29, 3.22MB/s]\u001b[A\nDownloading:  79%|███████▊  | 347M/440M [01:33<00:31, 3.00MB/s]\u001b[A\nDownloading:  79%|███████▉  | 347M/440M [01:33<00:27, 3.36MB/s]\u001b[A\nDownloading:  79%|███████▉  | 348M/440M [01:33<00:25, 3.61MB/s]\u001b[A\nDownloading:  79%|███████▉  | 348M/440M [01:33<00:24, 3.82MB/s]\u001b[A\nDownloading:  79%|███████▉  | 349M/440M [01:33<00:23, 3.87MB/s]\u001b[A\nDownloading:  79%|███████▉  | 349M/440M [01:33<00:24, 3.66MB/s]\u001b[A\nDownloading:  79%|███████▉  | 350M/440M [01:33<00:22, 3.99MB/s]\u001b[A\nDownloading:  79%|███████▉  | 350M/440M [01:33<00:22, 4.01MB/s]\u001b[A\nDownloading:  80%|███████▉  | 350M/440M [01:34<00:23, 3.84MB/s]\u001b[A\nDownloading:  80%|███████▉  | 351M/440M [01:34<00:23, 3.86MB/s]\u001b[A\nDownloading:  80%|███████▉  | 351M/440M [01:34<00:24, 3.67MB/s]\u001b[A\nDownloading:  80%|███████▉  | 352M/440M [01:34<00:22, 4.01MB/s]\u001b[A\nDownloading:  80%|███████▉  | 352M/440M [01:34<00:21, 4.05MB/s]\u001b[A\nDownloading:  80%|████████  | 353M/440M [01:34<00:22, 3.93MB/s]\u001b[A\nDownloading:  80%|████████  | 353M/440M [01:34<00:19, 4.37MB/s]\u001b[A\nDownloading:  80%|████████  | 354M/440M [01:34<00:19, 4.37MB/s]\u001b[A\nDownloading:  80%|████████  | 354M/440M [01:34<00:20, 4.22MB/s]\u001b[A\nDownloading:  81%|████████  | 355M/440M [01:35<00:18, 4.55MB/s]\u001b[A\nDownloading:  81%|████████  | 355M/440M [01:35<00:18, 4.53MB/s]\u001b[A\nDownloading:  81%|████████  | 356M/440M [01:35<00:18, 4.63MB/s]\u001b[A\nDownloading:  81%|████████  | 356M/440M [01:35<00:18, 4.65MB/s]\u001b[A\nDownloading:  81%|████████  | 357M/440M [01:35<00:17, 4.77MB/s]\u001b[A\nDownloading:  81%|████████  | 357M/440M [01:35<00:18, 4.62MB/s]\u001b[A\nDownloading:  81%|████████  | 358M/440M [01:35<00:18, 4.59MB/s]\u001b[A\nDownloading:  81%|████████▏ | 358M/440M [01:35<00:16, 4.91MB/s]\u001b[A\nDownloading:  81%|████████▏ | 359M/440M [01:35<00:17, 4.73MB/s]\u001b[A\nDownloading:  82%|████████▏ | 359M/440M [01:35<00:16, 4.91MB/s]\u001b[A\nDownloading:  82%|████████▏ | 360M/440M [01:36<00:15, 5.07MB/s]\u001b[A\nDownloading:  82%|████████▏ | 360M/440M [01:36<00:16, 4.73MB/s]\u001b[A\nDownloading:  82%|████████▏ | 361M/440M [01:36<00:22, 3.53MB/s]\u001b[A\nDownloading:  82%|████████▏ | 361M/440M [01:36<00:28, 2.75MB/s]\u001b[A\nDownloading:  82%|████████▏ | 362M/440M [01:36<00:25, 3.07MB/s]\u001b[A\nDownloading:  82%|████████▏ | 362M/440M [01:36<00:25, 3.08MB/s]\u001b[A\nDownloading:  82%|████████▏ | 363M/440M [01:37<00:23, 3.25MB/s]\u001b[A\nDownloading:  82%|████████▏ | 363M/440M [01:37<00:25, 3.07MB/s]\u001b[A\nDownloading:  82%|████████▏ | 363M/440M [01:37<00:25, 3.03MB/s]\u001b[A\nDownloading:  83%|████████▎ | 364M/440M [01:37<00:27, 2.85MB/s]\u001b[A\nDownloading:  83%|████████▎ | 364M/440M [01:37<00:26, 2.87MB/s]\u001b[A\nDownloading:  83%|████████▎ | 364M/440M [01:37<00:31, 2.45MB/s]\u001b[A\nDownloading:  83%|████████▎ | 364M/440M [01:37<00:31, 2.40MB/s]\u001b[A\nDownloading:  83%|████████▎ | 365M/440M [01:37<00:35, 2.15MB/s]\u001b[A\nDownloading:  83%|████████▎ | 365M/440M [01:38<00:36, 2.09MB/s]\u001b[A\nDownloading:  83%|████████▎ | 365M/440M [01:38<00:33, 2.26MB/s]\u001b[A\nDownloading:  83%|████████▎ | 365M/440M [01:38<00:35, 2.14MB/s]\u001b[A\nDownloading:  83%|████████▎ | 366M/440M [01:38<00:40, 1.85MB/s]\u001b[A\nDownloading:  83%|████████▎ | 366M/440M [01:38<00:38, 1.96MB/s]\u001b[A\nDownloading:  83%|████████▎ | 366M/440M [01:38<00:46, 1.58MB/s]\u001b[A\nDownloading:  83%|████████▎ | 366M/440M [01:38<00:42, 1.75MB/s]\u001b[A\nDownloading:  83%|████████▎ | 367M/440M [01:39<00:40, 1.83MB/s]\u001b[A\nDownloading:  83%|████████▎ | 367M/440M [01:39<00:47, 1.54MB/s]\u001b[A\nDownloading:  83%|████████▎ | 367M/440M [01:39<00:46, 1.57MB/s]\u001b[A\nDownloading:  83%|████████▎ | 367M/440M [01:39<00:41, 1.75MB/s]\u001b[A\nDownloading:  83%|████████▎ | 368M/440M [01:39<00:38, 1.88MB/s]\u001b[A\nDownloading:  84%|████████▎ | 368M/440M [01:39<00:54, 1.34MB/s]\u001b[A\nDownloading:  84%|████████▎ | 368M/440M [01:39<00:58, 1.24MB/s]\u001b[A\nDownloading:  84%|████████▎ | 368M/440M [01:40<00:55, 1.29MB/s]\u001b[A\nDownloading:  84%|████████▎ | 369M/440M [01:40<00:57, 1.26MB/s]\u001b[A\nDownloading:  84%|████████▍ | 369M/440M [01:40<00:49, 1.45MB/s]\u001b[A\nDownloading:  84%|████████▍ | 369M/440M [01:40<00:45, 1.56MB/s]\u001b[A\nDownloading:  84%|████████▍ | 369M/440M [01:40<00:40, 1.78MB/s]\u001b[A\nDownloading:  84%|████████▍ | 370M/440M [01:40<00:41, 1.71MB/s]\u001b[A\nDownloading:  84%|████████▍ | 370M/440M [01:40<00:40, 1.74MB/s]\u001b[A\nDownloading:  84%|████████▍ | 370M/440M [01:41<00:54, 1.30MB/s]\u001b[A\nDownloading:  84%|████████▍ | 370M/440M [01:41<00:47, 1.49MB/s]\u001b[A\nDownloading:  84%|████████▍ | 370M/440M [01:41<00:41, 1.69MB/s]\u001b[A\nDownloading:  84%|████████▍ | 371M/440M [01:41<00:58, 1.19MB/s]\u001b[A\nDownloading:  84%|████████▍ | 371M/440M [01:41<00:57, 1.21MB/s]\u001b[A\nDownloading:  84%|████████▍ | 371M/440M [01:42<00:56, 1.24MB/s]\u001b[A\nDownloading:  84%|████████▍ | 371M/440M [01:42<00:50, 1.38MB/s]\u001b[A\nDownloading:  84%|████████▍ | 371M/440M [01:42<00:48, 1.43MB/s]\u001b[A\nDownloading:  84%|████████▍ | 372M/440M [01:42<00:52, 1.32MB/s]\u001b[A\nDownloading:  84%|████████▍ | 372M/440M [01:42<00:53, 1.28MB/s]\u001b[A\nDownloading:  84%|████████▍ | 372M/440M [01:42<00:47, 1.45MB/s]\u001b[A\nDownloading:  85%|████████▍ | 372M/440M [01:42<00:50, 1.34MB/s]\u001b[A\nDownloading:  85%|████████▍ | 372M/440M [01:42<00:46, 1.47MB/s]\u001b[A\nDownloading:  85%|████████▍ | 373M/440M [01:43<00:47, 1.43MB/s]\u001b[A\nDownloading:  85%|████████▍ | 373M/440M [01:43<00:50, 1.33MB/s]\u001b[A\nDownloading:  85%|████████▍ | 373M/440M [01:43<00:58, 1.15MB/s]\u001b[A\nDownloading:  85%|████████▍ | 373M/440M [01:43<00:56, 1.20MB/s]\u001b[A\nDownloading:  85%|████████▍ | 373M/440M [01:43<01:46, 632kB/s]\u001b[A\nDownloading:  85%|████████▍ | 373M/440M [01:43<01:30, 744kB/s]\u001b[A\nDownloading:  85%|████████▍ | 373M/440M [01:44<01:20, 838kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:44<01:15, 885kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:44<01:37, 687kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:44<01:51, 596kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:45<03:03, 363kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:45<04:23, 252kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:45<03:19, 332kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:45<03:44, 295kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:46<03:52, 285kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:46<03:51, 286kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:46<03:23, 325kB/s]\u001b[A\nDownloading:  85%|████████▍ | 374M/440M [01:46<03:03, 361kB/s]\u001b[A\nDownloading:  85%|████████▌ | 374M/440M [01:46<02:46, 396kB/s]\u001b[A\nDownloading:  85%|████████▌ | 374M/440M [01:46<02:48, 391kB/s]\u001b[A\nDownloading:  85%|████████▌ | 375M/440M [01:47<02:24, 458kB/s]\u001b[A\nDownloading:  85%|████████▌ | 375M/440M [01:47<02:06, 522kB/s]\u001b[A\nDownloading:  85%|████████▌ | 375M/440M [01:47<01:54, 572kB/s]\u001b[A\nDownloading:  85%|████████▌ | 375M/440M [01:47<01:44, 627kB/s]\u001b[A\nDownloading:  85%|████████▌ | 375M/440M [01:47<01:32, 705kB/s]\u001b[A\nDownloading:  85%|████████▌ | 375M/440M [01:47<01:25, 765kB/s]\u001b[A\nDownloading:  85%|████████▌ | 375M/440M [01:47<01:14, 873kB/s]\u001b[A\nDownloading:  85%|████████▌ | 375M/440M [01:47<01:07, 959kB/s]\u001b[A\nDownloading:  85%|████████▌ | 375M/440M [01:47<01:00, 1.08MB/s]\u001b[A\nDownloading:  85%|████████▌ | 376M/440M [01:48<01:32, 700kB/s]\u001b[A\nDownloading:  85%|████████▌ | 376M/440M [01:48<01:17, 838kB/s]\u001b[A\nDownloading:  85%|████████▌ | 376M/440M [01:48<01:06, 976kB/s]\u001b[A\nDownloading:  85%|████████▌ | 376M/440M [01:48<00:56, 1.14MB/s]\u001b[A\nDownloading:  85%|████████▌ | 376M/440M [01:48<00:50, 1.28MB/s]\u001b[A\nDownloading:  85%|████████▌ | 376M/440M [01:48<00:44, 1.43MB/s]\u001b[A\nDownloading:  86%|████████▌ | 377M/440M [01:48<00:39, 1.60MB/s]\u001b[A\nDownloading:  86%|████████▌ | 377M/440M [01:48<00:39, 1.60MB/s]\u001b[A\nDownloading:  86%|████████▌ | 377M/440M [01:49<00:39, 1.60MB/s]\u001b[A\nDownloading:  86%|████████▌ | 378M/440M [01:49<00:32, 1.92MB/s]\u001b[A\nDownloading:  86%|████████▌ | 378M/440M [01:49<00:30, 2.05MB/s]\u001b[A\nDownloading:  86%|████████▌ | 378M/440M [01:49<00:31, 2.00MB/s]\u001b[A\nDownloading:  86%|████████▌ | 378M/440M [01:49<00:26, 2.35MB/s]\u001b[A\nDownloading:  86%|████████▌ | 379M/440M [01:49<00:24, 2.50MB/s]\u001b[A\nDownloading:  86%|████████▌ | 379M/440M [01:49<00:22, 2.69MB/s]\u001b[A\nDownloading:  86%|████████▌ | 380M/440M [01:49<00:20, 2.98MB/s]\u001b[A\nDownloading:  86%|████████▋ | 380M/440M [01:49<00:18, 3.32MB/s]\u001b[A\nDownloading:  86%|████████▋ | 380M/440M [01:50<00:18, 3.33MB/s]\u001b[A\nDownloading:  86%|████████▋ | 381M/440M [01:50<00:18, 3.26MB/s]\u001b[A\nDownloading:  87%|████████▋ | 381M/440M [01:50<00:15, 3.76MB/s]\u001b[A\nDownloading:  87%|████████▋ | 382M/440M [01:50<00:15, 3.76MB/s]\u001b[A\nDownloading:  87%|████████▋ | 382M/440M [01:50<00:15, 3.78MB/s]\u001b[A\nDownloading:  87%|████████▋ | 383M/440M [01:50<00:15, 3.85MB/s]\u001b[A\nDownloading:  87%|████████▋ | 383M/440M [01:50<00:14, 4.06MB/s]\u001b[A\nDownloading:  87%|████████▋ | 384M/440M [01:50<00:13, 4.20MB/s]\u001b[A\nDownloading:  87%|████████▋ | 384M/440M [01:50<00:12, 4.34MB/s]\u001b[A\nDownloading:  87%|████████▋ | 385M/440M [01:50<00:12, 4.61MB/s]\u001b[A\nDownloading:  87%|████████▋ | 385M/440M [01:51<00:11, 4.64MB/s]\u001b[A\nDownloading:  88%|████████▊ | 386M/440M [01:51<00:11, 4.87MB/s]\u001b[A\nDownloading:  88%|████████▊ | 386M/440M [01:51<00:11, 4.71MB/s]\u001b[A\nDownloading:  88%|████████▊ | 387M/440M [01:51<00:11, 4.80MB/s]\u001b[A\nDownloading:  88%|████████▊ | 387M/440M [01:51<00:11, 4.61MB/s]\u001b[A\nDownloading:  88%|████████▊ | 388M/440M [01:51<00:11, 4.53MB/s]\u001b[A\nDownloading:  88%|████████▊ | 388M/440M [01:51<00:10, 4.76MB/s]\u001b[A\nDownloading:  88%|████████▊ | 389M/440M [01:51<00:10, 4.78MB/s]\u001b[A\nDownloading:  88%|████████▊ | 389M/440M [01:51<00:10, 4.89MB/s]\u001b[A\nDownloading:  88%|████████▊ | 390M/440M [01:52<00:10, 4.78MB/s]\u001b[A\nDownloading:  89%|████████▊ | 390M/440M [01:52<00:10, 4.63MB/s]\u001b[A\nDownloading:  89%|████████▊ | 391M/440M [01:52<00:10, 4.57MB/s]\u001b[A\nDownloading:  89%|████████▉ | 391M/440M [01:52<00:11, 4.42MB/s]\u001b[A\nDownloading:  89%|████████▉ | 392M/440M [01:52<00:13, 3.70MB/s]\u001b[A\nDownloading:  89%|████████▉ | 392M/440M [01:52<00:12, 3.97MB/s]\u001b[A\nDownloading:  89%|████████▉ | 393M/440M [01:52<00:11, 4.09MB/s]\u001b[A\nDownloading:  89%|████████▉ | 393M/440M [01:52<00:11, 4.15MB/s]\u001b[A\nDownloading:  89%|████████▉ | 394M/440M [01:53<00:10, 4.37MB/s]\u001b[A\nDownloading:  90%|████████▉ | 394M/440M [01:53<00:09, 4.75MB/s]\u001b[A\nDownloading:  90%|████████▉ | 395M/440M [01:53<00:10, 4.38MB/s]\u001b[A\nDownloading:  90%|████████▉ | 395M/440M [01:53<00:10, 4.18MB/s]\u001b[A\nDownloading:  90%|████████▉ | 396M/440M [01:53<00:09, 4.50MB/s]\u001b[A\nDownloading:  90%|████████▉ | 396M/440M [01:53<00:09, 4.64MB/s]\u001b[A\nDownloading:  90%|█████████ | 397M/440M [01:53<00:11, 3.91MB/s]\u001b[A\nDownloading:  90%|█████████ | 397M/440M [01:53<00:09, 4.35MB/s]\u001b[A\nDownloading:  90%|█████████ | 398M/440M [01:53<00:09, 4.65MB/s]\u001b[A\nDownloading:  90%|█████████ | 398M/440M [01:54<00:09, 4.36MB/s]\u001b[A\nDownloading:  91%|█████████ | 399M/440M [01:54<00:09, 4.20MB/s]\u001b[A\nDownloading:  91%|█████████ | 399M/440M [01:54<00:09, 4.24MB/s]\u001b[A\nDownloading:  91%|█████████ | 400M/440M [01:54<00:09, 4.41MB/s]\u001b[A\nDownloading:  91%|█████████ | 400M/440M [01:54<00:09, 4.03MB/s]\u001b[A\nDownloading:  91%|█████████ | 401M/440M [01:54<00:11, 3.60MB/s]\u001b[A\nDownloading:  91%|█████████ | 401M/440M [01:54<00:11, 3.46MB/s]\u001b[A\nDownloading:  91%|█████████ | 401M/440M [01:54<00:12, 3.19MB/s]\u001b[A\nDownloading:  91%|█████████ | 402M/440M [01:55<00:11, 3.46MB/s]\u001b[A\nDownloading:  91%|█████████▏| 402M/440M [01:55<00:10, 3.69MB/s]\u001b[A\nDownloading:  91%|█████████▏| 403M/440M [01:55<00:10, 3.77MB/s]\u001b[A\nDownloading:  92%|█████████▏| 403M/440M [01:55<00:09, 3.85MB/s]\u001b[A\nDownloading:  92%|█████████▏| 404M/440M [01:55<00:09, 4.02MB/s]\u001b[A\nDownloading:  92%|█████████▏| 404M/440M [01:55<00:09, 3.85MB/s]\u001b[A\nDownloading:  92%|█████████▏| 404M/440M [01:55<00:09, 3.89MB/s]\u001b[A\nDownloading:  92%|█████████▏| 405M/440M [01:55<00:08, 4.10MB/s]\u001b[A\nDownloading:  92%|█████████▏| 405M/440M [01:55<00:08, 4.05MB/s]\u001b[A\nDownloading:  92%|█████████▏| 406M/440M [01:55<00:08, 4.32MB/s]\u001b[A\nDownloading:  92%|█████████▏| 406M/440M [01:56<00:08, 4.26MB/s]\u001b[A\nDownloading:  92%|█████████▏| 407M/440M [01:56<00:07, 4.42MB/s]\u001b[A\nDownloading:  92%|█████████▏| 407M/440M [01:56<00:07, 4.22MB/s]\u001b[A\nDownloading:  93%|█████████▎| 408M/440M [01:56<00:06, 4.92MB/s]\u001b[A\nDownloading:  93%|█████████▎| 409M/440M [01:56<00:06, 5.06MB/s]\u001b[A\nDownloading:  93%|█████████▎| 409M/440M [01:56<00:06, 5.12MB/s]\u001b[A\nDownloading:  93%|█████████▎| 410M/440M [01:56<00:06, 4.61MB/s]\u001b[A\nDownloading:  93%|█████████▎| 410M/440M [01:56<00:06, 4.94MB/s]\u001b[A\nDownloading:  93%|█████████▎| 411M/440M [01:56<00:06, 4.75MB/s]\u001b[A\nDownloading:  93%|█████████▎| 411M/440M [01:57<00:06, 4.51MB/s]\u001b[A\nDownloading:  93%|█████████▎| 412M/440M [01:57<00:06, 4.48MB/s]\u001b[A\nDownloading:  94%|█████████▎| 412M/440M [01:57<00:06, 4.26MB/s]\u001b[A\nDownloading:  94%|█████████▎| 413M/440M [01:57<00:06, 4.13MB/s]\u001b[A\nDownloading:  94%|█████████▍| 413M/440M [01:57<00:06, 4.43MB/s]\u001b[A\nDownloading:  94%|█████████▍| 414M/440M [01:57<00:06, 4.29MB/s]\u001b[A\nDownloading:  94%|█████████▍| 414M/440M [01:57<00:06, 4.04MB/s]\u001b[A\nDownloading:  94%|█████████▍| 415M/440M [01:57<00:06, 4.00MB/s]\u001b[A\nDownloading:  94%|█████████▍| 415M/440M [01:58<00:07, 3.20MB/s]\u001b[A\nDownloading:  94%|█████████▍| 415M/440M [01:58<00:07, 3.51MB/s]\u001b[A\nDownloading:  94%|█████████▍| 416M/440M [01:58<00:07, 3.17MB/s]\u001b[A\nDownloading:  94%|█████████▍| 416M/440M [01:58<00:08, 2.77MB/s]\u001b[A\nDownloading:  95%|█████████▍| 417M/440M [01:58<00:08, 2.98MB/s]\u001b[A\nDownloading:  95%|█████████▍| 417M/440M [01:58<00:07, 2.95MB/s]\u001b[A\nDownloading:  95%|█████████▍| 417M/440M [01:58<00:07, 3.06MB/s]\u001b[A\nDownloading:  95%|█████████▍| 418M/440M [01:58<00:07, 3.04MB/s]\u001b[A\nDownloading:  95%|█████████▍| 418M/440M [01:59<00:06, 3.31MB/s]\u001b[A\nDownloading:  95%|█████████▍| 418M/440M [01:59<00:08, 2.52MB/s]\u001b[A\nDownloading:  95%|█████████▌| 419M/440M [01:59<00:08, 2.59MB/s]\u001b[A\nDownloading:  95%|█████████▌| 419M/440M [01:59<00:08, 2.51MB/s]\u001b[A\nDownloading:  95%|█████████▌| 419M/440M [01:59<00:08, 2.62MB/s]\u001b[A\nDownloading:  95%|█████████▌| 420M/440M [01:59<00:08, 2.52MB/s]\u001b[A\nDownloading:  95%|█████████▌| 420M/440M [01:59<00:08, 2.47MB/s]\u001b[A\nDownloading:  95%|█████████▌| 420M/440M [02:00<00:09, 2.15MB/s]\u001b[A\nDownloading:  95%|█████████▌| 420M/440M [02:00<00:09, 2.15MB/s]\u001b[A\nDownloading:  95%|█████████▌| 421M/440M [02:00<00:09, 1.99MB/s]\u001b[A\nDownloading:  96%|█████████▌| 421M/440M [02:00<00:10, 1.94MB/s]\u001b[A\nDownloading:  96%|█████████▌| 421M/440M [02:00<00:10, 1.78MB/s]\u001b[A\nDownloading:  96%|█████████▌| 421M/440M [02:00<00:14, 1.33MB/s]\u001b[A\nDownloading:  96%|█████████▌| 421M/440M [02:00<00:12, 1.57MB/s]\u001b[A\nDownloading:  96%|█████████▌| 422M/440M [02:00<00:10, 1.78MB/s]\u001b[A\nDownloading:  96%|█████████▌| 422M/440M [02:01<00:10, 1.71MB/s]\u001b[A\nDownloading:  96%|█████████▌| 422M/440M [02:01<00:13, 1.36MB/s]\u001b[A\nDownloading:  96%|█████████▌| 423M/440M [02:01<00:10, 1.67MB/s]\u001b[A\nDownloading:  96%|█████████▌| 423M/440M [02:01<00:09, 1.85MB/s]\u001b[A\nDownloading:  96%|█████████▌| 423M/440M [02:01<00:07, 2.17MB/s]\u001b[A\nDownloading:  96%|█████████▌| 424M/440M [02:01<00:06, 2.43MB/s]\u001b[A\nDownloading:  96%|█████████▋| 424M/440M [02:01<00:06, 2.74MB/s]\u001b[A\nDownloading:  96%|█████████▋| 424M/440M [02:01<00:05, 3.14MB/s]\u001b[A\nDownloading:  96%|█████████▋| 425M/440M [02:02<00:04, 3.59MB/s]\u001b[A\nDownloading:  97%|█████████▋| 425M/440M [02:02<00:03, 3.86MB/s]\u001b[A\nDownloading:  97%|█████████▋| 426M/440M [02:02<00:03, 4.08MB/s]\u001b[A\nDownloading:  97%|█████████▋| 427M/440M [02:02<00:03, 4.40MB/s]\u001b[A\nDownloading:  97%|█████████▋| 427M/440M [02:02<00:02, 4.79MB/s]\u001b[A\nDownloading:  97%|█████████▋| 428M/440M [02:02<00:02, 4.85MB/s]\u001b[A\nDownloading:  97%|█████████▋| 428M/440M [02:02<00:02, 5.03MB/s]\u001b[A\nDownloading:  97%|█████████▋| 429M/440M [02:02<00:02, 4.58MB/s]\u001b[A\nDownloading:  97%|█████████▋| 429M/440M [02:02<00:02, 4.67MB/s]\u001b[A\nDownloading:  98%|█████████▊| 430M/440M [02:03<00:02, 4.51MB/s]\u001b[A\nDownloading:  98%|█████████▊| 430M/440M [02:03<00:02, 4.58MB/s]\u001b[A\nDownloading:  98%|█████████▊| 431M/440M [02:03<00:02, 4.82MB/s]\u001b[A\nDownloading:  98%|█████████▊| 431M/440M [02:03<00:01, 4.83MB/s]\u001b[A\nDownloading:  98%|█████████▊| 432M/440M [02:03<00:01, 5.08MB/s]\u001b[A\nDownloading:  98%|█████████▊| 432M/440M [02:03<00:01, 5.25MB/s]\u001b[A\nDownloading:  98%|█████████▊| 433M/440M [02:03<00:01, 4.92MB/s]\u001b[A\nDownloading:  98%|█████████▊| 433M/440M [02:03<00:01, 4.65MB/s]\u001b[A\nDownloading:  99%|█████████▊| 434M/440M [02:03<00:01, 4.20MB/s]\u001b[A\nDownloading:  99%|█████████▊| 434M/440M [02:04<00:01, 4.12MB/s]\u001b[A\nDownloading:  99%|█████████▊| 435M/440M [02:04<00:01, 4.36MB/s]\u001b[A\nDownloading:  99%|█████████▉| 435M/440M [02:04<00:01, 4.44MB/s]\u001b[A\nDownloading:  99%|█████████▉| 436M/440M [02:04<00:01, 4.24MB/s]\u001b[A\nDownloading:  99%|█████████▉| 436M/440M [02:04<00:01, 3.52MB/s]\u001b[A\nDownloading:  99%|█████████▉| 437M/440M [02:04<00:00, 3.93MB/s]\u001b[A\nDownloading:  99%|█████████▉| 437M/440M [02:04<00:00, 4.11MB/s]\u001b[A\nDownloading:  99%|█████████▉| 438M/440M [02:04<00:00, 4.46MB/s]\u001b[A\nDownloading: 100%|█████████▉| 438M/440M [02:04<00:00, 4.69MB/s]\u001b[A\nDownloading: 100%|█████████▉| 439M/440M [02:05<00:00, 4.66MB/s]\u001b[A\nDownloading: 100%|█████████▉| 439M/440M [02:05<00:00, 4.80MB/s]\u001b[A\nDownloading: 100%|██████████| 440M/440M [02:05<00:00, 3.51MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "BertForMaskedLM(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (cls): BertOnlyMLMHead(\n    (predictions): BertLMPredictionHead(\n      (transform): BertPredictionHeadTransform(\n        (dense): Linear(in_features=768, out_features=768, bias=True)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n      (decoder): Linear(in_features=768, out_features=30522, bias=True)\n    )\n  )\n)"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "bert_model = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model)\n",
    "model = BertForMaskedLM.from_pretrained(bert_model)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = 'I like blogkersx'\n",
    "error_word = 'blogkersx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_word_mask = input_text.replace(error_word, '[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = f'[CLS] {replaced_word_mask} [SEP]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_text = tokenizer.tokenize(text)\n",
    "masked_index = [i for i, x in enumerate(tokenize_text) if x == '[MASK]'][0]\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenize_text)\n",
    "segment_ids = [0]*len(tokenize_text)\n",
    "token_tensor = torch.tensor([indexed_tokens])\n",
    "segment_tensor = torch.tensor([segment_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "     outputs = model(token_tensor, segment_tensor)\n",
    "     predictions = torch.argmax(outputs[0][0]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'[UNK]'"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "3"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "[i for i, x in enumerate(tokenize_text) if x == '[MASK]'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['.', '!', ';', '?', '...', '|', ',', '-', 'it', 'and']"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(list(predicted_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskPrediction():\n",
    "    def __init__(self) -> None:\n",
    "        self.bert_model = 'bert-large-uncased'\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_model)\n",
    "        self.model = BertForMaskedLM.from_pretrained(self.bert_model)\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_bert_candidates(self, input_text, error_words, max_predictions=10):\n",
    "        list_candidates_bert = []\n",
    "        if error_words:\n",
    "            input_text_split = input_text.split(' ')\n",
    "            if len(input_text_split) > 3:\n",
    "                for word, error_word in zip(input_text.split(), error_words):\n",
    "                    if error_word:\n",
    "                        replace_word_mask = input_text.replace(word, '[MASK]')\n",
    "                        text = f'[CLS]{replace_word_mask} [SEP] {input_text} [SEP] '\n",
    "                        tokenize_text = self.tokenizer.tokenize(text)\n",
    "                        masked_index = [i for i, x in enumerate(tokenize_text) if x == '[MASK]'][0]\n",
    "                        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenize_text)\n",
    "                        segment_ids = [0]*len(tokenize_text)\n",
    "                        token_tensor = torch.tensor([indexed_tokens])\n",
    "                        segment_tensor = torch.tensor([segment_ids])\n",
    "                        with torch.no_grad():\n",
    "                            outputs = self.model(token_tensor, token_type_ids = segment_tensor)\n",
    "                            predictions = outputs[0][0][masked_index]\n",
    "                        predicted_ids = torch.argsort(predictions, descending=True)[:max_predictions]\n",
    "                        predicted_tokens = self.tokenizer.convert_ids_to_tokens(list(predicted_ids))\n",
    "                        list_candidates_bert.append((word, predicted_tokens))\n",
    "                return list_candidates_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordfreq"
   ]
  }
 ]
}